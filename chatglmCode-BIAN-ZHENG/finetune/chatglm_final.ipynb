{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d7ab94",
      "metadata": {
        "id": "c2d7ab94"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/THUDM/ChatGLM-6B.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FxL6UMB2vF0s",
      "metadata": {
        "id": "FxL6UMB2vF0s"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd3d436",
      "metadata": {
        "id": "7fd3d436"
      },
      "outputs": [],
      "source": [
        "!pip install -r ChatGLM-6B/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80638fb4",
      "metadata": {
        "id": "80638fb4"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_chinese nltk jieba datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PhZEprIzFVEh",
      "metadata": {
        "id": "PhZEprIzFVEh"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/THUDM/chatglm-6b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f07cc69",
      "metadata": {
        "id": "2f07cc69"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb49f4f",
      "metadata": {
        "id": "edb49f4f"
      },
      "outputs": [],
      "source": [
        "# # 加载模型\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# model_path = \"chatglm-6b\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
        "# # model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ad4909",
      "metadata": {
        "id": "03ad4909"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "# # 准备提示语\n",
        "# prompt = \"What is (vertigo) paroymsal  positional vertigo?\"\n",
        "\n",
        "# # 使用 IPython.display 流式打印模型输出\n",
        "# for response, history in model.stream_chat(\n",
        "#         tokenizer, prompt, history=[]):\n",
        "#     clear_output(wait=True)\n",
        "#     display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wPPooLxwv_Dk",
      "metadata": {
        "id": "wPPooLxwv_Dk"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68892314",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68892314",
        "outputId": "e760befb-d7fb-42eb-aa0e-b8ac3da75cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-21 14:19:23.796014: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-21 14:19:23.796085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-21 14:19:23.798099: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-21 14:19:24.808398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "04/21/2024 14:19:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/21/2024 14:19:26 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.02,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/chatglm-6b-finetune/runs/Apr21_14-19-26_a45e710ba288,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=50,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=500,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=output/chatglm-6b-finetune,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/chatglm-6b-finetune,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
            "  warnings.warn(\n",
            "Generating train split: 2025 examples [00:00, 52253.97 examples/s]\n",
            "[INFO|configuration_utils.py:666] 2024-04-21 14:19:27,044 >> loading configuration file chatglm-6b/config.json\n",
            "[WARNING|configuration_auto.py:905] 2024-04-21 14:19:27,044 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "[INFO|configuration_utils.py:666] 2024-04-21 14:19:27,086 >> loading configuration file chatglm-6b/config.json\n",
            "[INFO|configuration_utils.py:720] 2024-04-21 14:19:27,087 >> Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"chatglm-6b\",\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
            "  },\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"gmask_token_id\": 130001,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"inner_hidden_size\": 16384,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"mask_token_id\": 130000,\n",
            "  \"max_sequence_length\": 2048,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"position_encoding_2d\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 130528\n",
            "}\n",
            "\n",
            "[WARNING|tokenization_auto.py:652] 2024-04-21 14:19:27,087 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "[INFO|tokenization_utils_base.py:1800] 2024-04-21 14:19:27,127 >> loading file ice_text.model\n",
            "[INFO|tokenization_utils_base.py:1800] 2024-04-21 14:19:27,127 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1800] 2024-04-21 14:19:27,127 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1800] 2024-04-21 14:19:27,127 >> loading file tokenizer_config.json\n",
            "[WARNING|auto_factory.py:456] 2024-04-21 14:19:27,386 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "[INFO|modeling_utils.py:2400] 2024-04-21 14:19:27,465 >> loading weights file chatglm-6b/pytorch_model.bin.index.json\n",
            "[INFO|configuration_utils.py:575] 2024-04-21 14:19:27,466 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.27.1\"\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 8/8 [00:12<00:00,  1.60s/it]\n",
            "[INFO|modeling_utils.py:3032] 2024-04-21 14:19:40,673 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
            "\n",
            "[WARNING|modeling_utils.py:3034] 2024-04-21 14:19:40,673 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|modeling_utils.py:2690] 2024-04-21 14:19:40,743 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on train dataset: 100% 2025/2025 [00:01<00:00, 1148.41 examples/s]\n",
            "input_ids [526, 107, 19, 6552, 12518, 14, 5, 33191, 19671, 10722, 130009, 3014, 419, 54613, 31, 130001, 130004, 5, 59038, 53928, 25233, 54613, 19, 222, 29594, 14, 107, 145, 101, 100, 237, 511, 2185, 101, 54613, 31, 100, 8297, 16472, 109, 120, 22, 203, 16360, 126, 109, 100, 916, 101, 142, 709, 107, 16360, 7, 5, 59038, 53928, 25233, 54613, 2185, 3539, 5133, 101, 5587, 103, 7067, 48252, 7, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "inputs What is (vertigo) paroymsal  positional vertigo? Benign paroxysmal positional vertigo (BPPV) is one of the most common causes of vertigo? the sudden sensation that you're spinning or that the inside of your head is spinning. Benign paroxysmal positional vertigo causes brief episodes of mild to intense dizziness.\n",
            "label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 59038, 53928, 25233, 54613, 19, 222, 29594, 14, 107, 145, 101, 100, 237, 511, 2185, 101, 54613, 31, 100, 8297, 16472, 109, 120, 22, 203, 16360, 126, 109, 100, 916, 101, 142, 709, 107, 16360, 7, 5, 59038, 53928, 25233, 54613, 2185, 3539, 5133, 101, 5587, 103, 7067, 48252, 7, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
            "labels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> Benign paroxysmal positional vertigo (BPPV) is one of the most common causes of vertigo? the sudden sensation that you're spinning or that the inside of your head is spinning. Benign paroxysmal positional vertigo causes brief episodes of mild to intense dizziness.<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "  0% 0/500 [00:00<?, ?it/s]04/21/2024 14:19:46 - WARNING - transformers_modules.chatglm-6b.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.5337, 'learning_rate': 0.018000000000000002, 'epoch': 0.45}\n",
            "{'loss': 0.563, 'learning_rate': 0.016, 'epoch': 0.9}\n",
            "{'loss': 0.4144, 'learning_rate': 0.013999999999999999, 'epoch': 1.35}\n",
            "{'loss': 0.3174, 'learning_rate': 0.012, 'epoch': 1.8}\n",
            "{'loss': 0.2534, 'learning_rate': 0.01, 'epoch': 2.25}\n",
            "{'loss': 0.2088, 'learning_rate': 0.008, 'epoch': 2.7}\n",
            "{'loss': 0.1617, 'learning_rate': 0.006, 'epoch': 3.15}\n",
            "{'loss': 0.1517, 'learning_rate': 0.004, 'epoch': 3.6}\n",
            "{'loss': 0.1214, 'learning_rate': 0.002, 'epoch': 4.04}\n",
            "{'loss': 0.1277, 'learning_rate': 0.0, 'epoch': 4.49}\n",
            "100% 500/500 [19:13<00:00,  2.30s/it]Saving PrefixEncoder\n",
            "[INFO|configuration_utils.py:457] 2024-04-21 14:38:59,879 >> Configuration saved in output/chatglm-6b-finetune/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:362] 2024-04-21 14:38:59,880 >> Configuration saved in output/chatglm-6b-finetune/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:1762] 2024-04-21 14:39:00,104 >> Model weights saved in output/chatglm-6b-finetune/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2163] 2024-04-21 14:39:00,105 >> tokenizer config file saved in output/chatglm-6b-finetune/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2170] 2024-04-21 14:39:00,105 >> Special tokens file saved in output/chatglm-6b-finetune/checkpoint-500/special_tokens_map.json\n",
            "{'train_runtime': 1154.3962, 'train_samples_per_second': 6.93, 'train_steps_per_second': 0.433, 'train_loss': 0.3853183317184448, 'epoch': 4.49}\n",
            "100% 500/500 [19:14<00:00,  2.31s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =       4.49\n",
            "  train_loss               =     0.3853\n",
            "  train_runtime            = 0:19:14.39\n",
            "  train_samples            =       1777\n",
            "  train_samples_per_second =       6.93\n",
            "  train_steps_per_second   =      0.433\n"
          ]
        }
      ],
      "source": [
        "# P-tuning v2\n",
        "!PRE_SEQ_LEN=128 && LR=2e-2 && CUDA_VISIBLE_DEVICES=0 python3 ChatGLM-6B/ptuning/main.py \\\n",
        "    --do_train \\\n",
        "    --train_file final.csv \\\n",
        "    --prompt_column question \\\n",
        "    --response_column answer \\\n",
        "    --overwrite_cache \\\n",
        "    --model_name_or_path chatglm-6b \\\n",
        "    --output_dir output/chatglm-6b-finetune \\\n",
        "    --overwrite_output_dir \\\n",
        "    --max_source_length 64 \\\n",
        "    --max_target_length 64 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --predict_with_generate \\\n",
        "    --max_steps 500 \\\n",
        "    --logging_steps 50 \\\n",
        "    --save_steps 500 \\\n",
        "    --learning_rate $LR \\\n",
        "    --pre_seq_len $PRE_SEQ_LEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98aaa8fd",
      "metadata": {
        "id": "98aaa8fd"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "# import os\n",
        "\n",
        "# model_path = \"chatglm-6b\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "# # Fine-tuning 后的表现测试\n",
        "# config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=128)\n",
        "# model = AutoModel.from_pretrained(model_path, config=config, trust_remote_code=True)\n",
        "# # 此处使用你的 ptuning 工作目录\n",
        "# prefix_state_dict = torch.load(os.path.join(\"output/chatglm-6b-finetune/checkpoint-500\", \"pytorch_model.bin\"))\n",
        "# new_prefix_state_dict = {}\n",
        "# for k, v in prefix_state_dict.items():\n",
        "#     new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
        "# model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
        "\n",
        "# model = model.half().cuda()\n",
        "# model.transformer.prefix_encoder.float()\n",
        "# model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c529585b",
      "metadata": {
        "id": "c529585b"
      },
      "outputs": [],
      "source": [
        "# response, history = model.chat(tokenizer, \"What is (vertigo) paroymsal  positional vertigo?\", history=[])\n",
        "# response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19cb089",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d19cb089",
        "outputId": "52ef81f6-049c-4c64-c79e-077c12edef99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: output/chatglm-6b-finetune/ (stored 0%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/ (stored 0%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/generation_config.json (deflated 29%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/quantization.py (deflated 49%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/rng_state.pth (deflated 25%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/tokenizer_config.json (deflated 47%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/configuration_chatglm.py (deflated 68%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/pytorch_model.bin (deflated 7%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/tokenization_chatglm.py (deflated 75%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/training_args.bin (deflated 50%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/modeling_chatglm.py (deflated 77%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/special_tokens_map.json (deflated 45%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/ice_text.model (deflated 53%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/optimizer.pt (deflated 6%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/config.json (deflated 53%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/trainer_state.json (deflated 73%)\n",
            "  adding: output/chatglm-6b-finetune/checkpoint-500/scheduler.pt (deflated 56%)\n",
            "  adding: output/chatglm-6b-finetune/train_results.json (deflated 40%)\n",
            "  adding: output/chatglm-6b-finetune/runs/ (stored 0%)\n",
            "  adding: output/chatglm-6b-finetune/runs/Apr21_14-19-26_a45e710ba288/ (stored 0%)\n",
            "  adding: output/chatglm-6b-finetune/runs/Apr21_14-19-26_a45e710ba288/events.out.tfevents.1713709186.a45e710ba288.3417.0 (deflated 59%)\n",
            "  adding: output/chatglm-6b-finetune/runs/Apr21_14-19-26_a45e710ba288/1713709186.1761851/ (stored 0%)\n",
            "  adding: output/chatglm-6b-finetune/runs/Apr21_14-19-26_a45e710ba288/1713709186.1761851/events.out.tfevents.1713709186.a45e710ba288.3417.1 (deflated 63%)\n",
            "  adding: output/chatglm-6b-finetune/all_results.json (deflated 40%)\n",
            "  adding: output/chatglm-6b-finetune/trainer_state.json (deflated 73%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r newchatglmFinetune.zip ./output/chatglm-6b-finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cFrD19VSMOcu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFrD19VSMOcu",
        "outputId": "6ec8d477-78b9-403b-97db-39b03aaa5f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zuYn11wgMV7e",
      "metadata": {
        "id": "zuYn11wgMV7e"
      },
      "outputs": [],
      "source": [
        "!mv newchatglmFinetune.zip /content/drive/MyDrive/7102/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1203.48852,
      "end_time": "2023-06-29T04:10:31.363906",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-06-29T03:50:27.875386",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}